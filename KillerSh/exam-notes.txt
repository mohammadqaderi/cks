#Question-1
extract the certificate of user restricted@infra-prod and write it decoded to /opt/course/1/cert.
Solution:
k config view --raw -ojsonpath="{.users[2].user.client-certificate-data}" | base64 -d > /opt/course/1/cert

--------------------------------------------------------------------------------------------------------------------

#Question-2
Falco
- Find a Pod running image nginx which creates unwanted package management processes inside its container
- Find a Pod running image httpd which modifies /etc/passwd.
- Save the Falco logs for case 1 under /opt/course/2/falco.log in format:
  time-with-nanosconds,container-id,container-name,user-name
- No other information should be in any line. Collect the logs for at least 30 seconds.
- Afterwards, remove the threads (both 1 and 2) by scaling the replicas of the Deployments that control
  the offending Pods down to 0.

Solution:
- Using falco process, let's find the containers first:
  journalctl -fu falco | grep nginx
- After finding the container id, let's view it using crictl:
  crictl ps -id 7a5ea6a080d1
- Finding its pod:
  crictl pods -id 7a5ea6a080d1 -> First Pod is webapi-6cfddcd6f4-ftxg4 in Namespace team-blue.

- Finding the second pod using the same way:
  journalctl -fu falco | grep httpd
  crictl ps -id b1339d5cc2de
  crictl pods -id 595af943c3245 -> Second Pod is rating-service-68cbdf7b7-v2p6g in Namespace team-purple.

- Eliminate offending pods by scaling the deployments pods to 0
  k -n team-blue scale deploy webapi --replicas 0
  k -n team-purple scale deploy rating-service --replicas 0

- Create logs in correct format
  - Find the rule that we've to change from falco_rules.yaml and save it under falco_rules.local.yaml file:
  # Container is supposed to be immutable. Package management should be done in building the image.
  - rule: Launch Package Management Process in Container
    desc: Package management process ran inside container
    condition: >
      spawned_process
      and container
      and user.name != "_apt"
      and package_mgmt_procs
      and not package_mgmt_ancestor_procs
      and not user_known_package_manager_in_container
    output: >
      Package management process launched in container %evt.time,%container.id,%container.name,%user.name
    priority: ERROR
    tags: [process, mitre_persistence]

- Saving the Falco logs for case 1 under /opt/course/2/falco.log
  journalctl -fu falco | grep nginx -> Copy&paste the output into file /opt/course/2/falco.log

--------------------------------------------------------------------------------------------------------------------
#Question-3
Requirements:
- apiserver is Accessible through a NodePort Service
  Change the apiserver setup so that only accessible through a ClusterIP Service

Solution:
- Remove the following line from kube-apiserver.yaml file in kubernetes manifest:
  --kubernetes-service-node-port=31000
- Delete the kubernetes service (it'll lunch again with clusterIP type):
  kubectl delete svc kubernetes

--------------------------------------------------------------------------------------------------------------------
#Question-4
- Problem: There is Deployment container-host-hacker in Namespace team-red which mounts
  /run/containerd as a hostPath volume on the Node where it's running.
  This means that the Pod can access various data about other containers running on the same Node.

- Requirements:
  - configure Namespace team-red to enforce the baseline Pod Security Standard.
  - Once completed, delete the Pod of the Deployment mentioned above.
  - Check the ReplicaSet events and write the event/log lines containing the reason why the Pod
    isn't recreated into /opt/course/4/logs.

Solution:
- Edit team-red namespace: k edit ns team-red and add the following labels:
  labels:
    pod-security.kubernetes.io/enforce: baseline

- Delete the deployment pod:
  k -n team-red get pod
  k -n team-red delete pod container-host-hacker-dbf989777-wm8fc

- Check the deployment replicaSet and find why the pod isn't recreated:
  k -n team-red describe rs container-host-hacker-dbf989777
  - Copy the failure reason and paste it into /opt/course/4/logs
--------------------------------------------------------------------------------------------------------------------
#Question-6
- Note regarding verifying platform binaries:
  if two binaries files looks familiar in most characters, use diff command to see if the files are identical:
    - first check the shasum of the binary file:
      sha512sum kube-apiserver > kube-apiserver-shasum.txt
    - compare the two files content:
      diff kube-apiserver-original.txt kube-apiserver-shasum.txt
    - if there are any outputs then the sha512sum binary is not identical
--------------------------------------------------------------------------------------------------------------------

#Question-7
Requirements:
- The Open Policy Agent and Gatekeeper have been installed to, among other things,
  enforce blacklisting of certain image registries.
- Alter the existing constraint and/or template to also blacklist images from very-bad-registry.com.
- Test it by creating a single Pod using image very-bad-registry.com/image in Namespace default,
  it shouldn't work.
- You can also verify your changes by looking at the existing Deployment untrusted in Namespace default,
  it uses an image from the new untrusted source.
- The OPA constraint should throw violation messages for this one.

Solution:
- Edit the constraint template:
  k edit constrainttemplates blacklistimages
  - Add the following to rego section:
      images {
        image := input.review.object.spec.containers[_].image
        not startswith(image, "docker-fake.io/")
        not startswith(image, "google-gcr-fake.com/")
        not startswith(image, "very-bad-registry.com/") # ADD THIS LINE
      }
- Check if we can run pods with that image:
  k run opa-test --image=very-bad-registry.com/image -> Returns failure message

--------------------------------------------------------------------------------------------------------------------
#Question-11
Requirements:
- There is an existing Secret called database-access in Namespace team-green.
- Read the complete Secret content directly from ETCD (using etcdctl) and store it
  into /opt/course/11/etcd-secret-content.
- Write the plain and decoded Secret's value of key "pass" into /opt/course/11/database-password.

Solution:
- ETCD in Kubernetes stores data under /registry/
- ETCDCTL_API=3 etcdctl \
  --cert /etc/kubernetes/pki/apiserver-etcd-client.crt \
  --key /etc/kubernetes/pki/apiserver-etcd-client.key \
  --cacert /etc/kubernetes/pki/etcd/ca.crt get /registry/secrets/team-green/database-access
- Copy the secret content and save it into /opt/course/11/etcd-secret-content

- echo Y29uZmlkZW50aWFs | base64 -d > /opt/course/11/database-password
--------------------------------------------------------------------------------------------------------------------
#Question-12
Requirements:
- You're asked to investigate a possible permission escape in Namespace restricted.
- The context authenticates as user restricted which has only limited permissions and
  shouldn't be able to read Secret values.
- Try to find the password-key values of the Secrets secret1, secret2 and secret3 in Namespace restricted.
- Write the decoded plaintext values into files /opt/course/12/secret1, /opt/course/12/secret2
  and /opt/course/12/secret3.

Solution:
- This context is very restricted and can only read the secrets values, so we can't get resources:
  kubectl config use-context restricted@infra-prod
  k -n restricted get role,rolebinding,clusterrole,clusterrolebinding -> Forbidden
  k -n restricted get secret -> Forbidden
- Let's try to get all resources:
  k -n restricted get all -> returns 3 pods
- First pod is mounting secret1 under secret-volume, let's grep the secret:
  k -n restricted exec pod1-fd5d64b9c-pcx6q -- cat /etc/secret-volume/password > /opt/course/12/secret1
- Second pod uses secret2 as environment variable, let's grep the secret:
  k -n restricted exec pod2-6494f7699b-4hks5 -- env | grep PASS > PASSWORD=an-amazing
  echo an-amazing > /opt/course/12/secret2
- Secret 3 is not mounted for any pods, so we can try to use a Pod's ServiceAccount to access the third Secret:
  k -n restricted get pod -o yaml | grep automountServiceAccountToken
- Communicate with apiserver to get the secret details:
  curl https://kubernetes.default/api/v1/namespaces/restricted/secrets -H "Authorization: Bearer $(cat /run/secrets/kubernetes.io/serviceaccount/token)" -k
- echo cEVuRXRSYVRpT24tdEVzVGVSCg== | base64 -d > /opt/course/12/secret3

--------------------------------------------------------------------------------------------------------------------
#Question-13
Problem:
- There is a metadata service available at http://192.168.100.21:32000 on which Nodes can reach sensitive data,
  like cloud credentials for initialisation.
- By default, all Pods in the cluster also have access to this endpoint.
- The DevSecOps team has asked you to restrict access to this metadata server.

Requirements
- In Namespace metadata-access:
 - Create a NetworkPolicy named metadata-deny which prevents egress to 192.168.100.21 for all Pods but
   still allows access to everything else
 - Create a NetworkPolicy named metadata-allow which allows Pods having label role: metadata-accessor
   to access endpoint 192.168.100.21

Solution:
- metadata_deny.yaml:
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: metadata-deny
  namespace: metadata-access
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 192.168.100.21/32

- metadata-allow.yaml:
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: metadata-allow
  namespace: metadata-access
spec:
  podSelector:
    matchLabels:
      role: metadata-accessor
  policyTypes:
  - Egress
  egress:
  - to:
    - ipBlock:
        cidr: 192.168.100.21/32

- kubectl apply -f metadata-deny.yaml
- kubectl apply -f metadata-allow.yaml
--------------------------------------------------------------------------------------------------------------------
#Question-14
Problem:
- There are Pods in Namespace team-yellow.
- A security investigation noticed that some processes running in these Pods are using the Syscall kill,
 which is forbidden by a Team Yellow internal policy.

Requirement:
Find the offending Pod(s) and remove these by reducing the replicas of the parent Deployment to 0.

Solution:
- We've to use crictl to find the pods and inspect their processes:
  crictl pods --name collector1
  crictl pods --name collector2
  crictl pods --name collector3
- Finding pod collector1 processes:
  crictl inspect 9ea02422f8660 | grep args -A1 -> returns args that execute file name collector1-process
  ps aux | grep collector1-process -> Returns the process id
- Using strace to find Sycalls:
  strace -p 35039
- Collector1 is causing these issues, let's scale its deployment replicas to 0:
  k -n team-yellow scale deploy collector1 --replicas 0
- Collector2 and Collector3 are looks alright
--------------------------------------------------------------------------------------------------------------------
#Question-18
Problem:
- Namespace security contains five Secrets of type Opaque which can be considered highly confidential.
- The latest Incident-Prevention-Investigation revealed that ServiceAccount p.auster had too broad access
  to the cluster for some time. This SA should've never had access to any Secrets in that Namespace.
Requirements:
- Find out which Secrets in Namespace security this SA did access by looking at the Audit Logs under /opt/course/18/audit.log.
- Change the password to any new string of only those Secrets that were accessed by this SA.

Solution:
- Check the secrets under /opt/course/18/audit.log
  cat audit.log | grep "p.auster" | grep Secret | grep get | jq
- We found two secrets: vault-token and mysql-admin
- Changing the secrets values:
  echo new-vault-pass | base64 > bmV3LXZhdWx0LXBhc3MK
  k -n security edit secret vault-token
  echo new-mysql-pass | base64 > bmV3LW15c3FsLXBhc3MK
  k -n security edit secret mysql-admin
--------------------------------------------------------------------------------------------------------------------
#Question-20
Requirements:
- The cluster is running Kubernetes 1.27.6, update it to 1.28.2.
- Use apt package manager and kubeadm for this.
- Use ssh cluster3-controlplane1 and ssh cluster3-node1 to connect to the instances.

Solution:
# Control plane node upgrade:
- Drain control plane node before upgrade:
  k drain cluster3-controlplane1 --ignore-daemonsets
- SSH into cluster3-controlplane1:
  ssh cluster3-controlplane1
- Check the kubectl and kubelet versions:
  kubelet --version
  kubeadm version
- Install kubeadm required version:
  apt-mark unhold kubeadm
  apt-mark hold kubectl kubelet
  apt install kubeadm=1.28.2-00
  apt-mark hold kubeadm
- Use kubeadm to upgrade the node:
  kubeadm upgrade plan
  kubeadm upgrade apply v1.28.2
- Upgrade kubectl and kubelet:
  apt update
  apt-mark unhold kubelet kubectl
  apt install kubelet=1.28.2-00 kubectl=1.28.2-00
  apt-mark hold kubelet kubectl
- Restart the kubelet service:
  service kubelet restart
- Uncordon the control plane node:
  k uncordon cluster3-controlplane1

#cluster3-node1 node upgrade:
- Drain the node:
  k drain cluster3-node1 --ignore-daemonsets
- Upgrade the kubeadm:
  apt update
  apt-mark hold kubectl kubelet
  apt install kubeadm=1.28.2-00
  apt-mark hold kubeadm
- Use kubeadm to upgrade the node:
  kubeadm upgrade node
- Upgrade kubelet and kubectl:
  apt install kubelet=1.28.2-00 kubectl=1.28.2-00
- Restart the kubelet service:
  service kubelet restart
- Uncordon the node:
  k uncordon cluster3-node1
------------------------------------------------------------------------------------------------
#Question-22
Requirements:
- The Release Engineering Team has shared some YAML manifests and Dockerfiles with you to review
- As a container security expert, you are asked to perform a manual static analysis and find out possible security issues with respect to unwanted credential exposure.
  Running processes as root is of no concern in this task.
- Write the filenames which have issues into /opt/course/22/security-issues.
- Scan the 3 Dockerfiles and 7 kubernetes resources
- All the files are exist in Question-22-files directory

Solution:
#Dockerfile-mysql
- File Dockerfile-mysql might look innocent on first look. It copies a file secret-token over,
uses it and deletes it afterwards. But because of the way Docker works, every RUN, COPY and ADD command
creates a new layer and every layer is persisted in the image.
- This means even if the file secret-token get's deleted in layer Z, it's still included with the image in layer X and Y. In this case it would be better to use for example variables passed to Docker.
- Write the filename to security-issues: echo Dockerfile-mysql >> /opt/course/22/security-issues

#deployment-redis.yaml
- Credentials in logs is never a good idea.
- Write the filename to security-issues: echo deployment-redis.yaml >> /opt/course/22/security-issues

#statefulset-nginx.yaml
- Secrets environment variables values are hardcoded.
- Write the filename to security-issues: echo statefulset-nginx.yaml >> /opt/course/22/security-issues

Rest of files are not risky
------------------------------------------------------------------------------------------------

#Preview-Question-1
Problems:
- A security scan result shows that there is an unknown miner process running on one of the Nodes in cluster3.
- The report states that the process is listening on port 6666.

Requirement: Kill the process and delete the binary.

Solution:
- Check the first node:
  ssh cluster1-controlplane1
  netstat -plnt | grep 6666 -> Returns nothing

- Check the second node:
  ssh cluster1-node1
  netstat -plnt | grep 6666
    -> tcp6       0      0 :::6666                 :::*        LISTEN      9591/system-atm
  - Check the process:
    lsof -i :6666
    -> system-at 9591 root    3u  IPv6  47760      0t0  TCP *:6666 (LISTEN)
  - Check the /proc directory for the full process path:
    ls /proc/9591/exe
    -> /proc/9591/exe -> /bin/system-atm
  - Kill the process:
    kill -9 9591
  - Remove the binary:
    rm /bin/system-atm